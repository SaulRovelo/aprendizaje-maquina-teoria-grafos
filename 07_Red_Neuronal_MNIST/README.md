# üß† Red Neuronal para Reconocimiento de Im√°genes (MNIST 0 y 1)

Este notebook implementa una **red neuronal simple en PyTorch** para reconocer los d√≠gitos **0 y 1** del conjunto **MNIST**, un cl√°sico dataset que contiene im√°genes de n√∫meros escritos a mano. Se muestra c√≥mo preparar los datos, definir el modelo, entrenarlo y evaluar su desempe√±o.

---

## üìò Conceptos generales

### üîπ Tensores

En PyTorch, toda la informaci√≥n ‚Äîim√°genes, etiquetas, pesos y gradientes‚Äî se representa como **tensores**.  
Un tensor es una estructura parecida a un arreglo de Numpy, pero optimizada para operaciones en GPU, lo que permite acelerar el entrenamiento.  

Por ejemplo, una imagen en escala de grises de 28x28 p√≠xeles se representa como un tensor con forma:  
```
(1, 28, 28)
```
donde:
- `1` indica el canal (grises),  
- `28x28` corresponde al tama√±o de la imagen.

---

### üîπ M√°scara de datos

El conjunto MNIST contiene im√°genes de los d√≠gitos del **0 al 9**, pero este notebook se enfoca en un **problema binario**: reconocer solo los d√≠gitos **0 y 1**.  

Para lograrlo, se aplica una **m√°scara booleana**, que filtra los datos y conserva √∫nicamente las muestras donde la etiqueta es 0 o 1:

```python
mascara = (targets == 0) | (targets == 1)
```

Esto simplifica el problema de clasificaci√≥n, reduci√©ndolo a dos clases posibles.

---

### üîπ Caracter√≠sticas (X) y Etiquetas (Y)

- **Caracter√≠sticas (X):** son las im√°genes de entrada, es decir, los p√≠xeles del n√∫mero escrito.  
- **Etiquetas (Y):** son las salidas esperadas o respuestas correctas del modelo (`0` o `1`).

Ambos se convierten a tipo **float** (`.float()`) para que puedan ser procesados correctamente por PyTorch durante el entrenamiento.

---

### üîπ Aplanamiento (Flatten)

Cada imagen original de 28x28 p√≠xeles se transforma en un vector de **784 valores** (`28*28`).  
Este proceso, llamado **aplanamiento**, permite conectar todos los p√≠xeles directamente a la capa de entrada de la red neuronal.

Ejemplo de transformaci√≥n:
```
Entrada:  (1, 28, 28)
Salida:   (1, 784)
```

De esta forma, cada imagen se convierte en una lista lineal de caracter√≠sticas lista para ser procesada por el modelo.

---

## ‚öôÔ∏è Definici√≥n del modelo y entrenamiento

El modelo se define con `nn.Sequential()`, una forma sencilla de construir redes en PyTorch agregando capas de manera secuencial.  
En este caso, el flujo del modelo es el siguiente:

1. **Flatten**: aplana la imagen (28x28 ‚Üí 784).  
2. **Linear (784 ‚Üí 512)**: primera capa totalmente conectada, que aprende representaciones intermedias.  
3. **Sigmoid**: introduce una activaci√≥n no lineal para permitir que el modelo aprenda relaciones m√°s complejas.  
4. **Linear (512 ‚Üí 1)**: capa de salida con una sola neurona, que representa la probabilidad de que la imagen pertenezca a la clase 1.  
5. **Sigmoid final**: convierte el resultado en un valor entre 0 y 1.

La **funci√≥n de p√©rdida** utilizada es `nn.BCELoss()` (Binary Cross Entropy), que mide el error entre las predicciones y las etiquetas reales en tareas binarias.  
El **optimizador** es `torch.optim.SGD`, que ajusta los pesos del modelo mediante **descenso del gradiente** con una tasa de aprendizaje definida (`lr=0.001`).

Durante el **entrenamiento**, el modelo aprende repitiendo un ciclo de pasos:

- Calcula las predicciones (`forward pass`).
- Eval√∫a el error con la funci√≥n de p√©rdida.
- Calcula los gradientes (`backward()`).
- Actualiza los pesos (`step()`).
- Repite el proceso por un n√∫mero determinado de iteraciones o √©pocas.

Este procedimiento permite que el modelo minimice la p√©rdida y mejore su precisi√≥n en las predicciones.

---

## üßæ Evaluaci√≥n y pruebas individuales

Una vez entrenado, el modelo se pone en modo evaluaci√≥n con `modelo.eval()`.  
Durante esta fase, se desactiva el c√°lculo de gradientes (`torch.no_grad()`) para hacer el proceso m√°s r√°pido y eficiente.

El conjunto de prueba se pasa por el modelo, que genera una **probabilidad** para cada imagen.  
Luego se aplica un **umbral de 0.5**:  
- Si la probabilidad es mayor a 0.5 ‚Üí se predice clase **1**  
- Si es menor o igual a 0.5 ‚Üí se predice clase **0**

Comparando las predicciones con las etiquetas verdaderas, se calcula la **precisi√≥n** del modelo, es decir, el porcentaje de aciertos sobre el total de im√°genes evaluadas.

Tambi√©n se incluye una **prueba individual**, en la cual se selecciona una imagen de ejemplo del conjunto de prueba.  
La imagen se muestra en pantalla junto con su etiqueta real, y el modelo predice si pertenece a la clase 0 o 1.  
Esto permite observar de manera visual c√≥mo el modelo clasifica ejemplos reales.

---

## üßÆ Funciones y m√≥dulos usados

- **`transforms.ToTensor()`** ‚Üí convierte las im√°genes en tensores y normaliza los valores a 0‚Äì1.  
- **`datasets.MNIST()`** ‚Üí carga o descarga el conjunto de datos MNIST.  
- **`.float()`** ‚Üí cambia el tipo de dato a flotante, necesario para operaciones con PyTorch.  
- **`.reshape()` / `.view()`** ‚Üí cambia la forma del tensor (por ejemplo, de 28x28 a 784).  
- **`nn.Sequential()`** ‚Üí construye un modelo capa por capa.  
- **`nn.Linear()`** ‚Üí capa lineal que realiza una combinaci√≥n lineal de las entradas.  
- **`nn.Sigmoid()`** ‚Üí funci√≥n de activaci√≥n que mapea valores a probabilidades (0‚Äì1).  
- **`nn.BCELoss()`** ‚Üí calcula la p√©rdida para clasificaci√≥n binaria.  
- **`torch.optim.SGD()`** ‚Üí optimizador de descenso por gradiente estoc√°stico.  
- **`.zero_grad()`** ‚Üí reinicia los gradientes antes de un nuevo paso de entrenamiento.  
- **`.backward()`** ‚Üí calcula los gradientes de la p√©rdida respecto a los pesos.  
- **`.step()`** ‚Üí actualiza los par√°metros del modelo seg√∫n los gradientes calculados.  
- **`modelo.eval()`** ‚Üí cambia el modelo a modo evaluaci√≥n.  
- **`torch.no_grad()`** ‚Üí desactiva el c√°lculo de gradientes durante la inferencia.  
- **`plt.imshow()`** ‚Üí muestra im√°genes en escala de grises.  
- **`plt.title()`** y **`plt.show()`** ‚Üí a√±aden t√≠tulo y visualizan la figura.

---

## ‚ú® Conclusi√≥n

Este notebook demuestra el ciclo completo de una red neuronal simple:  
**cargar, preparar, entrenar, evaluar y probar** un modelo de aprendizaje supervisado.  
A trav√©s del ejemplo de MNIST, se ilustran los conceptos clave del aprendizaje de m√°quina:  
**tensores, funciones de activaci√≥n, p√©rdida, optimizaci√≥n y evaluaci√≥n**, todo dentro de un flujo pr√°ctico y f√°cil de entender.
