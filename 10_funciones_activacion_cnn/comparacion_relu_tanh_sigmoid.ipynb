{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparaci√≥n de Funciones de Activaci√≥n en una CNN (MNIST)\n",
        "\n",
        "---\n",
        "\n",
        "## **Introducci√≥n**\n",
        "\n",
        "En el aprendizaje profundo, las **funciones de activaci√≥n** son un componente esencial de las redes neuronales, ya que permiten introducir **no linealidad** en los modelos y, con ello, la capacidad de **aprender patrones complejos**.\n",
        "\n",
        "Este notebook compara el comportamiento de tres funciones de activaci√≥n cl√°sicas:\n",
        "- **ReLU (Rectified Linear Unit)**  \n",
        "- **Tanh (Tangente hiperb√≥lica)**  \n",
        "- **Sigmoid**\n",
        "\n",
        "Todas son evaluadas dentro de la **misma red neuronal convolucional (CNN)** entrenada sobre el conjunto de datos **MNIST**, que contiene im√°genes en escala de grises de d√≠gitos escritos a mano (0‚Äì9).\n",
        "\n",
        "---\n",
        "\n",
        "## **Conceptos generales**\n",
        "\n",
        "- **Funci√≥n de activaci√≥n:**  \n",
        "  Determina c√≥mo responde cada neurona a la informaci√≥n que recibe. Su papel es introducir *no linealidad*, permitiendo que la red aprenda relaciones complejas.  \n",
        "\n",
        "- **CNN (Convolutional Neural Network):**  \n",
        "  Tipo de red dise√±ada para procesar datos con estructura espacial (como im√°genes). Utiliza *capas convolucionales* que detectan bordes, formas y patrones.\n",
        "\n",
        "- **MNIST:**  \n",
        "  Dataset de im√°genes de **d√≠gitos escritos a mano** (28√ó28 px) usado tradicionalmente para evaluar modelos de visi√≥n por computadora.\n",
        "\n",
        "- **Precisi√≥n (Accuracy):**  \n",
        "  Porcentaje de aciertos del modelo al clasificar im√°genes nuevas (de prueba).\n",
        "\n",
        "- **Saturaci√≥n de gradientes:**  \n",
        "  Problema com√∫n en redes profundas donde algunas funciones (como **Sigmoid**) reducen el valor del gradiente casi a cero, dificultando el aprendizaje.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Importaci√≥n de librer√≠as y configuraci√≥n del entorno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usando PyTorch v2.9.0+cpu\n",
            "Usando: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch                      # N√∫cleo de PyTorch (tensores, GPU, etc.)\n",
        "import torch.nn as nn             # Capas y m√≥dulos para redes neuronales\n",
        "import torch.optim as optim       # Algoritmos de optimizaci√≥n (SGD, Adam, etc.)\n",
        "import torchvision                # Datasets y utilidades para visi√≥n por computadora\n",
        "import torchvision.transforms as transforms  # Transformaciones para preprocesar im√°genes\n",
        "import time                       # Medir tiempo de entrenamiento\n",
        "import numpy as np                # Operaciones num√©ricas adicionales\n",
        "\n",
        "print(f\"Usando PyTorch v{torch.__version__}\")\n",
        "\n",
        "# Configurar Dispositivo (SOLO CUDA)\n",
        "# if torch.cuda.is_available():\n",
        "#     device = torch.device(\"cuda\")\n",
        "#     print(\"¬°√âxito! Usando dispositivo: cuda (GPU)\")\n",
        "# else:\n",
        "#     print(\"ERROR: CUDA no est√° disponible.\")\n",
        "#     print(\"Este script requiere una GPU NVIDIA.\")\n",
        "#     exit() # Salir si no hay CUDA\n",
        "\n",
        "# Selecci√≥n autom√°tica del dispositivo (GPU si est√° disponible, de lo contrario CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Usando:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Carga y preparaci√≥n del dataset (MNIST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n",
            "100.0%\n",
            "100.0%\n",
            "100.0%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# üìö 2. Carga y preparaci√≥n del dataset (MNIST)\n",
        "\n",
        "# Transformaciones que se aplican a cada imagen:\n",
        "# 1) ToTensor() ‚Üí convierte la imagen (0‚Äì255) en un tensor (0‚Äì1)\n",
        "# 2) Normalize() ‚Üí ajusta los valores a un rango [-1, 1]\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Cargar el dataset MNIST (d√≠gitos escritos a mano)\n",
        "# root: carpeta donde se almacenan los datos\n",
        "# train=True  ‚Üí conjunto de entrenamiento (60,000 im√°genes)\n",
        "# train=False ‚Üí conjunto de prueba (10,000 im√°genes)\n",
        "# download=True ‚Üí lo descarga si no existe\n",
        "# transform ‚Üí aplica las transformaciones definidas arriba\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                      download=True, transform=transform)\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                     download=True, transform=transform)\n",
        "\n",
        "# DataLoaders ‚Üí cargan los datos en mini-lotes (batches)\n",
        "# batch_size: cantidad de im√°genes por iteraci√≥n\n",
        "# shuffle=True ‚Üí mezcla los datos en cada √©poca\n",
        "# num_workers: hilos de carga (2 = m√°s r√°pido en CPU)\n",
        "BATCH_SIZE = 64\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
        "                                         shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Definici√≥n del modelo CNN profundo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# üß© 3. Definici√≥n del modelo CNN profundo\n",
        "\n",
        "def build_deep_cnn_model(activation_module):\n",
        "    \"\"\"\n",
        "    Crea una red neuronal convolucional (CNN) profunda.\n",
        "    \n",
        "    Par√°metros:\n",
        "      activation_module : m√≥dulo de activaci√≥n de PyTorch (nn.ReLU(), nn.Tanh(), nn.Sigmoid(), etc.)\n",
        "                          Define qu√© funci√≥n de activaci√≥n se aplicar√° entre capas.\n",
        "    \"\"\"\n",
        "    model = nn.Sequential(\n",
        "\n",
        "        # ---------- Bloque 1 ----------\n",
        "        nn.Conv2d(\n",
        "            in_channels=1,   # n√∫mero de canales de entrada (1 para im√°genes en escala de grises)\n",
        "            out_channels=16, # n√∫mero de filtros (mapas de caracter√≠sticas) que generar√° esta capa\n",
        "            kernel_size=3,   # tama√±o del filtro convolucional (3x3)\n",
        "            padding=1        # agrega un ‚Äúborde‚Äù de 1 p√≠xel para conservar el tama√±o original\n",
        "        ),\n",
        "        activation_module,\n",
        "\n",
        "        nn.Conv2d(\n",
        "            in_channels=16,  # entrada: 16 canales del paso anterior\n",
        "            out_channels=16, # salida: mantiene 16 canales\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        ),\n",
        "        activation_module,\n",
        "\n",
        "        nn.MaxPool2d(\n",
        "            kernel_size=2,   # reduce el tama√±o de la imagen a la mitad (2x2)\n",
        "            stride=2         # desplazamiento de la ventana del maxpool\n",
        "        ),  # salida: 16 x 14 x 14\n",
        "\n",
        "\n",
        "        # ---------- Bloque 2 ----------\n",
        "        nn.Conv2d(\n",
        "            in_channels=16, out_channels=32,\n",
        "            kernel_size=3, padding=1\n",
        "        ),\n",
        "        activation_module,\n",
        "\n",
        "        nn.Conv2d(\n",
        "            in_channels=32, out_channels=32,\n",
        "            kernel_size=3, padding=1\n",
        "        ),\n",
        "        activation_module,\n",
        "\n",
        "        nn.MaxPool2d(\n",
        "            kernel_size=2, stride=2\n",
        "        ),  # salida: 32 x 7 x 7\n",
        "\n",
        "\n",
        "        # ---------- Bloque 3 ----------\n",
        "        nn.Conv2d(\n",
        "            in_channels=32, out_channels=64,\n",
        "            kernel_size=3, padding=1\n",
        "        ),\n",
        "        activation_module,\n",
        "\n",
        "        nn.Conv2d(\n",
        "            in_channels=64, out_channels=64,\n",
        "            kernel_size=3, padding=1\n",
        "        ),\n",
        "        activation_module,\n",
        "\n",
        "        nn.Conv2d(\n",
        "            in_channels=64, out_channels=64,\n",
        "            kernel_size=3, padding=1\n",
        "        ),\n",
        "        activation_module,\n",
        "\n",
        "\n",
        "        # ---------- Clasificador ----------\n",
        "        nn.Flatten(),  # convierte el tensor 3D (64x7x7) en un vector 1D (3136 valores)\n",
        "\n",
        "        nn.Linear(\n",
        "            in_features=64 * 7 * 7,  # n√∫mero de entradas (neuronas aplanadas)\n",
        "            out_features=128         # n√∫mero de salidas (neuronas en la capa oculta)\n",
        "        ),\n",
        "        activation_module,\n",
        "\n",
        "        nn.Linear(\n",
        "            in_features=128,  # n√∫mero de entradas (de la capa anterior)\n",
        "            out_features=10   # n√∫mero de clases de salida (d√≠gitos 0‚Äì9)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Mueve el modelo al dispositivo correcto (CPU o GPU)\n",
        "    return model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚öôÔ∏è 4. Funciones de entrenamiento y evaluaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Funci√≥n de Entrenamiento\n",
        "def train_model(model, trainloader, criterion, optimizer, epochs=3):\n",
        "    \"\"\"\n",
        "    Entrena el modelo con los datos de entrenamiento.\n",
        "    \n",
        "    Par√°metros:\n",
        "      model      : red neuronal a entrenar\n",
        "      trainloader: DataLoader con los lotes (batches) de entrenamiento\n",
        "      criterion  : funci√≥n de p√©rdida (por ej. CrossEntropyLoss)\n",
        "      optimizer  : algoritmo que ajusta los pesos (Adam, SGD, etc.)\n",
        "      epochs     : n√∫mero de √©pocas de entrenamiento\n",
        "    \"\"\"\n",
        "    model.train()  # activa el modo entrenamiento (permite dropout, batchnorm, etc.)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0  # acumulador de p√©rdida por √©poca\n",
        "\n",
        "        # i = √≠ndice del batch | data = (inputs, labels)\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "            # Mover los tensores al dispositivo (CPU o GPU)\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()        # reinicia los gradientes acumulados\n",
        "            outputs = model(inputs)      # inferencia (forward pass)\n",
        "            loss = criterion(outputs, labels)  # calcula la p√©rdida\n",
        "            loss.backward()              # retropropagaci√≥n (calcula gradientes)\n",
        "            optimizer.step()             # actualiza los pesos del modelo\n",
        "\n",
        "            running_loss += loss.item()  # acumula p√©rdida promedio\n",
        "\n",
        "        print(f\"  [√âpoca {epoch + 1}/{epochs}] P√©rdida: {running_loss / len(trainloader):.3f}\")\n",
        "\n",
        "# Funci√≥n de Evaluaci√≥n\n",
        "def evaluate_model(model, testloader):\n",
        "    \"\"\"\n",
        "    Eval√∫a el modelo en el conjunto de prueba.\n",
        "    \n",
        "    Par√°metros:\n",
        "      model      : red neuronal ya entrenada\n",
        "      testloader : DataLoader con los datos de prueba\n",
        "    \n",
        "    Retorna:\n",
        "      accuracy (%) del modelo sobre el conjunto de prueba\n",
        "    \"\"\"\n",
        "    model.eval()  # modo evaluaci√≥n (desactiva dropout y batchnorm)\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # torch.no_grad() ‚Üí desactiva el c√°lculo de gradientes (m√°s r√°pido y eficiente)\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)                # predicciones\n",
        "            _, predicted = torch.max(outputs.data, 1)  # √≠ndice de la clase con mayor probabilidad\n",
        "            total += labels.size(0)                # total de im√°genes evaluadas\n",
        "            correct += (predicted == labels).sum().item()  # conteo de aciertos\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Entrenamiento y comparaci√≥n de funciones de activaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Iniciando comparaci√≥n (Red PROFUNDA, 3 √©pocas)...\n",
            "\n",
            "--- ENTRENANDO CON: RELU ---\n",
            "  [√âpoca 1/3] P√©rdida: 0.259\n",
            "  [√âpoca 2/3] P√©rdida: 0.057\n",
            "  [√âpoca 3/3] P√©rdida: 0.041\n",
            "Tiempo: 88.45 segundos\n",
            "Precisi√≥n: 99.06 %\n",
            "\n",
            "--- ENTRENANDO CON: TANH ---\n",
            "  [√âpoca 1/3] P√©rdida: 0.198\n",
            "  [√âpoca 2/3] P√©rdida: 0.076\n",
            "  [√âpoca 3/3] P√©rdida: 0.063\n",
            "Tiempo: 94.50 segundos\n",
            "Precisi√≥n: 98.69 %\n",
            "\n",
            "--- ENTRENANDO CON: SIGMOID ---\n",
            "  [√âpoca 1/3] P√©rdida: 2.308\n",
            "  [√âpoca 2/3] P√©rdida: 2.303\n",
            "  [√âpoca 3/3] P√©rdida: 2.302\n",
            "Tiempo: 88.80 segundos\n",
            "Precisi√≥n: 11.35 %\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Diccionario con las funciones de activaci√≥n a probar.\n",
        "# La clave es el nombre (string) y el valor es el m√≥dulo de PyTorch.\n",
        "activations_to_test = {\n",
        "    'relu': nn.ReLU(),      # Rectified Linear Unit\n",
        "    'tanh': nn.Tanh(),      # Tangente hiperb√≥lica\n",
        "    'sigmoid': nn.Sigmoid() # Funci√≥n sigmoide\n",
        "}\n",
        "\n",
        "training_results = {}  # guardar√° los resultados finales de cada activaci√≥n\n",
        "EPOCHS = 3             # n√∫mero de √©pocas para todas las pruebas (igual para comparar)\n",
        "\n",
        "print(f\"\\nIniciando comparaci√≥n (Red PROFUNDA, {EPOCHS} √©pocas)...\\n\")\n",
        "\n",
        "# Iterar sobre cada activaci√≥n del diccionario\n",
        "for name, activation_module in activations_to_test.items():\n",
        "    print(f\"--- ENTRENANDO CON: {name.upper()} ---\")\n",
        "\n",
        "    # Construir el modelo con la activaci√≥n correspondiente\n",
        "    model = build_deep_cnn_model(activation_module)\n",
        "\n",
        "    # Funci√≥n de p√©rdida para clasificaci√≥n multiclase (usa logits sin softmax)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Optimizador Adam: ajusta los pesos del modelo en cada paso\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "    # Medir tiempo total de entrenamiento\n",
        "    start_time = time.time()\n",
        "    train_model(model, trainloader, criterion, optimizer, epochs=EPOCHS)\n",
        "    total_time = time.time() - start_time\n",
        "\n",
        "    # Evaluar el modelo en el conjunto de prueba\n",
        "    final_accuracy = evaluate_model(model, testloader)\n",
        "\n",
        "    # Guardar resultados para esta activaci√≥n\n",
        "    training_results[name] = {\n",
        "        \"time_seconds\": total_time,\n",
        "        \"final_accuracy\": final_accuracy\n",
        "    }\n",
        "\n",
        "    # Mostrar resultados intermedios\n",
        "    print(f\"Tiempo: {total_time:.2f} segundos\")\n",
        "    print(f\"Precisi√≥n: {final_accuracy:.2f} %\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Resultados finales de la comparaci√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- RESULTADOS (Red PROFUNDA) ---\n",
            "Entrenamiento sobre 3 √©pocas (Batch Size: 64)\n",
            "\n",
            "Funci√≥n    | Tiempo (seg)    | Precisi√≥n Final (%) \n",
            "--------------------------------------------------\n",
            "relu       | 88.45           | 99.06               \n",
            "tanh       | 94.50           | 98.69               \n",
            "sigmoid    | 88.80           | 11.35               \n"
          ]
        }
      ],
      "source": [
        "# Encabezado general\n",
        "print(\"--- RESULTADOS (Red PROFUNDA) ---\")\n",
        "print(f\"Entrenamiento sobre {EPOCHS} √©pocas (Batch Size: {BATCH_SIZE})\\n\")\n",
        "\n",
        "# Formato de tabla con columnas alineadas\n",
        "print(f\"{'Funci√≥n':<10} | {'Tiempo (seg)':<15} | {'Precisi√≥n Final (%)':<20}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Recorrer el diccionario con los resultados guardados\n",
        "for activation, results in training_results.items():\n",
        "    # Imprimir nombre de la activaci√≥n, tiempo y precisi√≥n formateados\n",
        "    print(f\"{activation:<10} | {results['time_seconds']:<15.2f} | {results['final_accuracy']:<20.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
