# üß© Comparaci√≥n de Funciones de Activaci√≥n en Redes Convolucionales (CNN)


## üéØ **Prop√≥sito**

Este notbook busca **analizar y comparar** c√≥mo distintas **funciones de activaci√≥n** afectan el **aprendizaje y desempe√±o** de una red neuronal convolucional (CNN) profunda entrenada sobre el conjunto de datos **MNIST** (d√≠gitos escritos a mano).

A trav√©s de un experimento controlado, se entrenan **tres modelos id√©nticos** ‚Äîcon la misma arquitectura, datos y optimizador‚Äî variando √∫nicamente la **funci√≥n de activaci√≥n interna**:

- **ReLU (Rectified Linear Unit)**  
- **Tanh (Tangente hiperb√≥lica)**  
- **Sigmoid**

---

## üß† **Contexto te√≥rico**

### üîπ ¬øQu√© es una funci√≥n de activaci√≥n?

Una **funci√≥n de activaci√≥n** define c√≥mo una neurona transforma su entrada en salida.  
Su objetivo es **introducir no linealidad**, permitiendo que la red neuronal aprenda **relaciones complejas** entre los datos.

Sin activaciones, una red ser√≠a solo una combinaci√≥n lineal de sus entradas, incapaz de aprender patrones no lineales (por ejemplo, distinguir curvas, letras o formas).

---

### üîπ Tipos comparados en este experimento

| Activaci√≥n | Ecuaci√≥n | Rango | Ventajas | Desventajas |
|-------------|-----------|--------|------------|--------------|
| **Sigmoid** | f(x) = 1 / (1 + exp(-x)) | (0, 1) | Salida en forma de probabilidad | Se satura (gradientes ‚âà 0) |
| **Tanh** | f(x) = tanh(x) | (-1, 1) | Centra las salidas en 0 | Tambi√©n puede saturar |
| **ReLU** | f(x) = max(0, x) | [0, ‚àû) | R√°pida, no satura en positivos | Puede ‚Äúmatar‚Äù neuronas negativas |

Cada una impacta de forma distinta la propagaci√≥n del gradiente y la velocidad de convergencia.

---

## üßÆ **Estructura del experimento**

1. **Dataset:**  
   Se utiliza **MNIST**, un conjunto con 70,000 im√°genes en escala de grises (28√ó28 px) de d√≠gitos del 0 al 9.  
   - 60,000 im√°genes para entrenamiento.  
   - 10,000 im√°genes para prueba.

2. **Preprocesamiento:**  
   Las im√°genes se convierten a tensores y se normalizan al rango **[-1, 1]**, adecuado para activaciones sim√©tricas como *tanh*.

3. **Arquitectura de la red (CNN profunda):**
   - 3 bloques convolucionales con filtros de 3√ó3 y *MaxPooling* (reduce tama√±o espacial).
   - Capas densas finales para clasificaci√≥n.
   - Activaci√≥n intercambiable (ReLU, Tanh o Sigmoid).

4. **Entrenamiento:**
   - Optimizaci√≥n con **Adam** (lr=0.001).
   - P√©rdida: **CrossEntropyLoss** (clasificaci√≥n multiclase).
   - Entrenamiento por 3 √©pocas sobre todos los datos.

5. **Evaluaci√≥n:**
   - Se mide el **tiempo total de entrenamiento**.
   - Se calcula la **precisi√≥n (accuracy)** sobre el conjunto de prueba.
   - Los resultados se guardan y se presentan en una tabla comparativa.

---

## ‚öôÔ∏è **Flujo general del c√≥digo**

1Ô∏è‚É£ Cargar librer√≠as y configurar PyTorch  
2Ô∏è‚É£ Descargar y normalizar el dataset MNIST  
3Ô∏è‚É£ Definir el modelo CNN (build_deep_cnn_model)  
4Ô∏è‚É£ Implementar funciones de entrenamiento y evaluaci√≥n  
5Ô∏è‚É£ Ejecutar el bucle de comparaci√≥n (ReLU, Tanh, Sigmoid)  
6Ô∏è‚É£ Mostrar resultados finales  

---

## üìä **Resultados esperados**

| Activaci√≥n | Tiempo (s) | Precisi√≥n (%) | Comportamiento |
|-------------|-------------|----------------|----------------|
| **ReLU** | R√°pido | 98‚Äì99% | Entrenamiento estable, buen gradiente |
| **Tanh** | Medio | 97‚Äì99% | Ligeramente m√°s lento, pero preciso |
| **Sigmoid** | Similar | ~10% | Falla: gradientes saturados (aprendizaje nulo) |

> Este comportamiento ilustra el fen√≥meno del **desvanecimiento del gradiente**, donde funciones como *Sigmoid* pierden informaci√≥n en redes profundas.

---

## üß© **Conclusiones**

- La **ReLU** domina en redes profundas: simple, eficiente y evita saturaci√≥n.  
- **Tanh** funciona bien en redes peque√±as o con datos centrados en cero.  
- **Sigmoid**, aunque √∫til en salidas binarias, no es adecuada para capas internas profundas.  
- Este tipo de experimentos son esenciales para entender **c√≥mo la elecci√≥n de activaci√≥n puede determinar el √©xito o fracaso del entrenamiento.**

---

‚úçÔ∏è **Autor:**  
Sa√∫l Rovelo L√≥pez  
Universidad Aut√≥noma Metropolitana ‚Äì Unidad Cuajimalpa  
Curso: *Aprendizaje de M√°quina aplicado a Teor√≠a de Gr√°ficas*
