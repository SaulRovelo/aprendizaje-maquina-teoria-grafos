{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neurona log√≠stica (PyTorch) \n",
        "\n",
        "## Objetivo\n",
        "Entrenar **una neurona log√≠stica**  para que aprenda la tabla l√≥gica **OR**.\n",
        "\n",
        "Dado un par de entradas binarias $( (x_1, x_2) )$, el modelo predice la **probabilidad** de que $( y = 1 )$ (verdadero).  \n",
        "Matem√°ticamente, la neurona calcula:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "z &= w_1 x_1 + w_2 x_2 + b \\\\\n",
        "\\hat{y} &= \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "Los par√°metros del modelo son:  \n",
        "$w_1, w_2$ ‚Äî **pesos** o par√°metros aprendidos,  \n",
        "$b$ ‚Äî **sesgo (bias)** que ajusta la frontera de decisi√≥n,  \n",
        "$\\sigma(z)$ ‚Äî **funci√≥n sigmoide** que mapea cualquier n√∫mero real al rango $(0,1)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usando dispositivo: cpu\n",
            "Iteraci√≥n 0 - P√©rdida: 0.7678\n",
            "Iteraci√≥n 1000 - P√©rdida: 0.0898\n",
            "Iteraci√≥n 2000 - P√©rdida: 0.0471\n",
            "Iteraci√≥n 3000 - P√©rdida: 0.0315\n",
            "Iteraci√≥n 4000 - P√©rdida: 0.0236\n",
            "Iteraci√≥n 5000 - P√©rdida: 0.0188\n",
            "Iteraci√≥n 6000 - P√©rdida: 0.0156\n",
            "Iteraci√≥n 7000 - P√©rdida: 0.0134\n",
            "Iteraci√≥n 8000 - P√©rdida: 0.0117\n",
            "Iteraci√≥n 9000 - P√©rdida: 0.0104\n",
            "\n",
            "üîπ Predicciones despu√©s de entrenar:\n",
            "tensor([[0.0205],\n",
            "        [0.9918],\n",
            "        [0.9918],\n",
            "        [1.0000]], grad_fn=<SigmoidBackward0>)\n",
            "\n",
            "üîπ Par√°metros aprendidos:\n",
            "0.weight: tensor([[8.6612, 8.6605]])\n",
            "0.bias: tensor([-3.8653])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# Elegir el dispositivo de c√≥mputo\n",
        "\n",
        "# torch.device() permite definir si el modelo y los tensores se ejecutan en:\n",
        "#  - \"cuda\" ‚Üí GPU (si est√° disponible)\n",
        "#  - \"cpu\"  ‚Üí procesador normal\n",
        "# torch.cuda.is_available() devuelve True si hay GPU compatible con CUDA.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "\n",
        "# 1Ô∏è. Definici√≥n de la capa lineal (modelo lineal)\n",
        "\n",
        "# nn.Linear(in_features, out_features)\n",
        "# Crea una transformaci√≥n lineal:  y = X¬∑W^T + b\n",
        "#  - in_features = n√∫mero de variables de entrada (x1, x2 ‚Üí 2)\n",
        "#  - out_features = n√∫mero de salidas (una sola neurona ‚Üí 1)\n",
        "# Internamente guarda dos par√°metros entrenables:\n",
        "#  - weight ‚Üí matriz de pesos (w1, w2)\n",
        "#  - bias   ‚Üí t√©rmino independiente b\n",
        "capa_lineal = nn.Linear(2, 1)\n",
        "\n",
        "\n",
        "# 2Ô∏è. Funci√≥n de activaci√≥n (sigmoide)\n",
        "\n",
        "# nn.Sigmoid() aplica la funci√≥n log√≠stica œÉ(z) = 1 / (1 + e^(-z))\n",
        "# Convierte el resultado lineal (z) en una probabilidad entre 0 y 1.\n",
        "# No recibe argumentos.\n",
        "activacion = nn.Sigmoid()\n",
        "\n",
        "\n",
        "# 3Ô∏è. Construcci√≥n del modelo secuencial\n",
        "\n",
        "# nn.Sequential() agrupa varias capas en orden:\n",
        "#   - Primero aplica la capa lineal (z = WX + b)\n",
        "#   - Luego la sigmoide (œÉ(z))\n",
        "# to(device) mueve el modelo al CPU o GPU definido antes.\n",
        "modelo = nn.Sequential(\n",
        "    capa_lineal,\n",
        "    activacion\n",
        ").to(device)\n",
        "\n",
        "\n",
        "# 4Ô∏è. Datos de entrenamiento (Tabla OR)\n",
        "\n",
        "# torch.tensor(data, device, dtype)\n",
        "#   - data: lista o arreglo de valores num√©ricos\n",
        "#   - device: en qu√© dispositivo se guardan (CPU o GPU)\n",
        "#   - dtype: tipo de dato (float32 ‚Üí n√∫mero decimal)\n",
        "#\n",
        "# Entradas X: todas las combinaciones de (x1, x2)\n",
        "# Salidas Y: resultados de x1 OR x2\n",
        "X = torch.tensor([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "], device=device, dtype=torch.float32)\n",
        "\n",
        "Y = torch.tensor([\n",
        "    [0],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1]\n",
        "], device=device, dtype=torch.float32)\n",
        "\n",
        "\n",
        "# 5Ô∏è. Funci√≥n de p√©rdida y optimizador\n",
        "\n",
        "# nn.BCELoss() ‚Üí Binary Cross Entropy Loss\n",
        "#   - Calcula la diferencia entre las predicciones y los valores reales.\n",
        "#   - Mide qu√© tan buena es la predicci√≥n de una probabilidad binaria.\n",
        "#   - F√≥rmula: L = -[y¬∑log(≈∑) + (1 - y)¬∑log(1 - ≈∑)]\n",
        "fn_perdida = nn.BCELoss()\n",
        "\n",
        "# torch.optim.SGD(params, lr)\n",
        "#   - Implementa el m√©todo del Gradiente Descendente Estoc√°stico.\n",
        "#   - params: par√°metros del modelo (pesos y sesgos)\n",
        "#   - lr: tasa de aprendizaje (learning rate)\n",
        "# Actualiza los par√°metros del modelo en cada iteraci√≥n:\n",
        "#   Œ∏ := Œ∏ - Œ∑ * ‚àáŒ∏ L\n",
        "optimizador = torch.optim.SGD(modelo.parameters(), lr=0.1)\n",
        "\n",
        "\n",
        "# 6Ô∏è. Ciclo de entrenamiento\n",
        "\n",
        "# Repite el proceso 10,000 veces para que el modelo aprenda (epochs)\n",
        "for epoch in range(10000):\n",
        "\n",
        "    # --- Forward pass ---\n",
        "    # Calcula las salidas del modelo para las entradas X\n",
        "    # Internamente: ≈∑ = œÉ(WX + b)\n",
        "    y_pred = modelo(X)\n",
        "\n",
        "    # --- Calcular p√©rdida ---\n",
        "    # Compara las predicciones (y_pred) con los valores reales (Y)\n",
        "    perdida = fn_perdida(y_pred, Y)\n",
        "    \n",
        "    # --- Limpiar gradientes ---\n",
        "    # Antes de calcular nuevos gradientes, se limpian los anteriores\n",
        "    optimizador.zero_grad()\n",
        "\n",
        "    # --- Backpropagation ---\n",
        "    # Calcula autom√°ticamente las derivadas parciales (gradientes)\n",
        "    # de la p√©rdida con respecto a cada par√°metro del modelo\n",
        "    perdida.backward()\n",
        "\n",
        "    # --- Actualizaci√≥n de par√°metros ---\n",
        "    # Modifica los pesos (w1, w2) y el sesgo (b)\n",
        "    # seg√∫n la direcci√≥n del gradiente y la tasa de aprendizaje\n",
        "    optimizador.step()\n",
        "    \n",
        "    # --- Monitoreo del entrenamiento ---\n",
        "    # Cada 1000 iteraciones imprime la p√©rdida para ver la convergencia\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Iteraci√≥n {epoch} - P√©rdida: {perdida.item():.4f}\")\n",
        "\n",
        "\n",
        "# 7Ô∏è. Predicciones finales\n",
        "\n",
        "# Despu√©s del entrenamiento, volvemos a pasar los datos por el modelo\n",
        "# para ver qu√© probabilidades produce.\n",
        "print(\"\\nüîπ Predicciones despu√©s de entrenar:\")\n",
        "print(modelo(X))\n",
        "# Deber√≠an aproximarse a:\n",
        "# [ [0.0], [0.99], [0.99], [1.0] ]\n",
        "\n",
        "\n",
        "# 8Ô∏è. Par√°metros finales (pesos y sesgo)\n",
        "\n",
        "# modelo.named_parameters() devuelve un iterable con:\n",
        "#  - el nombre del par√°metro (\"weight\" o \"bias\")\n",
        "#  - el valor entrenado (tensor con los pesos aprendidos)\n",
        "print(\"\\nüîπ Par√°metros aprendidos:\")\n",
        "for nombre, param in modelo.named_parameters():\n",
        "    print(f\"{nombre}: {param.data}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumen de las funciones usadas\n",
        "\n",
        "- **`torch.device(type)`**  \n",
        "  Define si se usar√° **CPU** o **GPU** para ejecutar el modelo y los tensores.  \n",
        "  Argumento: `\"cpu\"` o `\"cuda\"`.\n",
        "\n",
        "---\n",
        "\n",
        "- **`torch.cuda.is_available()`**  \n",
        "  Devuelve `True` si existe una **GPU compatible con CUDA** disponible en el sistema.  \n",
        "  *(Sin argumentos).*\n",
        "\n",
        "---\n",
        "\n",
        "- **`nn.Linear(in_features, out_features)`**  \n",
        "  Crea una capa lineal que aplica la transformaci√≥n:  \n",
        "  $$\n",
        "  y = W X + b\n",
        "  $$\n",
        "  donde \\( W \\) son los **pesos** y \\( b \\) el **sesgo**.  \n",
        "  Argumentos:  \n",
        "  - `in_features`: n√∫mero de entradas (columnas de \\( X \\)).  \n",
        "  - `out_features`: n√∫mero de salidas (neuronas de salida).\n",
        "\n",
        "---\n",
        "\n",
        "- **`nn.Sigmoid()`**  \n",
        "  Aplica la funci√≥n log√≠stica:  \n",
        "  $$\n",
        "  \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "  $$\n",
        "  Convierte un valor real \\( z \\) en una probabilidad entre 0 y 1.  \n",
        "  *(Sin argumentos).*\n",
        "\n",
        "---\n",
        "\n",
        "- **`nn.Sequential(...)`**  \n",
        "  Combina varias capas en orden, de modo que la salida de una es la entrada de la siguiente.  \n",
        "  Argumento: lista de capas (por ejemplo `[nn.Linear(), nn.Sigmoid()]`).\n",
        "\n",
        "---\n",
        "\n",
        "- **`torch.tensor(data, device, dtype)`**  \n",
        "  Crea un **tensor** (estructura de datos multidimensional).  \n",
        "  Argumentos:  \n",
        "  - `data`: lista o arreglo NumPy.  \n",
        "  - `device`: `\"cpu\"` o `\"cuda\"`.  \n",
        "  - `dtype`: tipo de dato, por ejemplo `torch.float32`.\n",
        "\n",
        "---\n",
        "\n",
        "- **`nn.BCELoss()`**  \n",
        "  Calcula la **p√©rdida de entrop√≠a cruzada binaria**, que mide la diferencia entre las probabilidades predichas y las reales:  \n",
        "  $$\n",
        "  L = -[y \\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})]\n",
        "  $$\n",
        "  *(Sin argumentos).*\n",
        "\n",
        "---\n",
        "\n",
        "- **`torch.optim.SGD(params, lr)`**  \n",
        "  Implementa el **Gradiente Descendente Estoc√°stico (SGD)**, actualizando los par√°metros seg√∫n:  \n",
        "  $$\n",
        "  \\theta := \\theta - \\eta \\nabla_\\theta L\n",
        "  $$\n",
        "  Argumentos:  \n",
        "  - `params`: par√°metros del modelo (pesos y sesgos).  \n",
        "  - `lr`: tasa de aprendizaje (\\( \\eta \\)).\n",
        "\n",
        "---\n",
        "\n",
        "- **`optimizer.zero_grad()`**  \n",
        "  Limpia los **gradientes acumulados** antes de calcular los nuevos.  \n",
        "  *(Sin argumentos).*\n",
        "\n",
        "---\n",
        "\n",
        "- **`loss.backward()`**  \n",
        "  Calcula los **gradientes** de la p√©rdida con respecto a cada par√°metro del modelo usando *backpropagation*.  \n",
        "  *(Sin argumentos).*\n",
        "\n",
        "---\n",
        "\n",
        "- **`optimizer.step()`**  \n",
        "  Actualiza los par√°metros (\\( W \\), \\( b \\)) usando los gradientes calculados.  \n",
        "  *(Sin argumentos).*\n",
        "\n",
        "---\n",
        "\n",
        "- **`model(X)`**  \n",
        "  Aplica el **modelo completo** (capa lineal + sigmoide) a las entradas \\( X \\) y devuelve las predicciones \\( \\hat{y} \\).  \n",
        "  Argumento: `X` ‚Äî tensor con los datos de entrada.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
