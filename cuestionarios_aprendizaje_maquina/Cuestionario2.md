
# ðŸ§© Cuestionario 2 de Repaso

### 1ï¸âƒ£ En una neurona con funciÃ³n lineal z y funciÃ³n de activaciÃ³n f(z), Â¿quÃ© representa geomÃ©tricamente que z=0?

âœ… **Respuesta:**  
Un **hiperplano que separa dos clases** en el espacio de entrada.

ðŸ’¡ **ExplicaciÃ³n:**  
El plano definido por z=0 actÃºa como **frontera de decisiÃ³n**, dividiendo las clases segÃºn el signo de z.

---

### 2ï¸âƒ£ Si y = g(x) y z = f(y), Â¿cuÃ¡l de las siguientes expresiones representa correctamente la derivada dz/dx segÃºn la regla de la cadena?

âœ… **Respuesta:**  
`dz/dx = dz/dy * dy/dx`

ðŸ’¡ **ExplicaciÃ³n:**  
La **regla de la cadena** multiplica derivadas parciales intermedias,  
expresando cÃ³mo cambia z con respecto a x a travÃ©s de y.

---

### 3ï¸âƒ£ Â¿CuÃ¡l de las siguientes operaciones construye una funciÃ³n indicadora en el intervalo [a,b] usando funciones escalÃ³n H(x)?

âœ… **Respuesta:**  
`Resta: H(xâˆ’a) âˆ’ H(xâˆ’b)`

ðŸ’¡ **ExplicaciÃ³n:**  
`H(xâˆ’a)` se activa en a, `H(xâˆ’b)` se apaga en b;  
su resta vale 1 solo dentro del intervalo [a,b].

---

### 4ï¸âƒ£ Â¿QuÃ© establece el Teorema de AproximaciÃ³n Universal?

âœ… **Respuesta:**  
Una **red con una sola capa oculta** y suficientes neuronas puede **aproximar cualquier funciÃ³n continua** en un intervalo.

ðŸ’¡ **ExplicaciÃ³n:**  
El teorema (Hornik y Cybenko, 1989) muestra que las redes neuronales pueden aproximar cualquier funciÃ³n continua,  
si tienen suficientes neuronas y una funciÃ³n de activaciÃ³n no lineal.

---

### 5ï¸âƒ£ En PyTorch, si una variable z depende de otra variable x, Â¿cÃ³mo se obtiene la derivada dz/dx?

âœ… **Respuesta:**  
Llamando a `z.backward()` y consultando el valor en `x.grad`.

ðŸ’¡ **ExplicaciÃ³n:**  
`backward()` realiza **backpropagation automÃ¡tica** y almacena la derivada de z respecto a x en `x.grad`.

---

### 6ï¸âƒ£ Â¿QuÃ© ocurre en PyTorch si intentas sumar dos tensores que estÃ¡n en diferentes dispositivos (uno en CPU y otro en GPU)?

âœ… **Respuesta:**  
Se genera un **error**, porque los tensores deben estar **en el mismo dispositivo**.

ðŸ’¡ **ExplicaciÃ³n:**  
PyTorch **no convierte automÃ¡ticamente** entre CPU y GPU.  
Es necesario mover manualmente los tensores con `.to('cuda')` o `.to('cpu')`.

---

### 7ï¸âƒ£ Â¿A quÃ© funciÃ³n se aproxima Ïƒ(wx) (la sigmoide) cuando w â†’ âˆž?

âœ… **Respuesta:**  
A una **funciÃ³n escalÃ³n (Heaviside)**.

ðŸ’¡ **ExplicaciÃ³n:**  
Cuando w crece, la sigmoide se comporta como un **escalÃ³n binario**:  
1 si x>0, 0 si x<0.

---

### 8ï¸âƒ£ Â¿CuÃ¡l es una limitaciÃ³n fundamental de usar una sola neurona para clasificaciÃ³n?

âœ… **Respuesta:**  
No puede **separar regiones no linealmente separables**.

ðŸ’¡ **ExplicaciÃ³n:**  
Una neurona define un **hiperplano lineal**, por lo que no puede resolver problemas como XOR.

---
V
### 9ï¸âƒ£ Â¿CuÃ¡l es la ventaja principal de realizar operaciones en GPU con PyTorch?

âœ… **Respuesta:**  
Permite **procesar grandes cantidades de datos mÃ¡s rÃ¡pido** que en CPU.

ðŸ’¡ **ExplicaciÃ³n:**  
Las GPU tienen miles de nÃºcleos para operaciones en paralelo,  
acelerando el entrenamiento de modelos de redes neuronales.

---
