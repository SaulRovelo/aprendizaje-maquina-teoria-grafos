{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß© Neurona log√≠stica (PyTorch) \n",
        "\n",
        "## üéØ Objetivo\n",
        "Entrenar **una neurona log√≠stica**  para que aprenda la tabla l√≥gica **OR**.\n",
        "\n",
        "Dado un par de entradas binarias $( (x_1, x_2) )$, el modelo predice la **probabilidad** de que $( y = 1 )$ (verdadero).  \n",
        "Matem√°ticamente, la neurona calcula:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "z &= w_1 x_1 + w_2 x_2 + b \\\\\n",
        "\\hat{y} &= \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "Los par√°metros del modelo son:  \n",
        "$w_1, w_2$ ‚Äî **pesos** o par√°metros aprendidos,  \n",
        "$b$ ‚Äî **sesgo (bias)** que ajusta la frontera de decisi√≥n,  \n",
        "$\\sigma(z)$ ‚Äî **funci√≥n sigmoide** que mapea cualquier n√∫mero real al rango $(0,1)$.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## üèóÔ∏è ¬øQu√© hace el c√≥digo?\n",
        "1. **Definimos el modelo**  \n",
        "   - `nn.Linear(2,1)` crea la transformaci√≥n lineal $( z = W X + b ) $ con 2 entradas y 1 salida.  \n",
        "   - `nn.Sigmoid()` aplica la funci√≥n log√≠stica $\\sigma(z)$.  \n",
        "   - Ambas se combinan con `nn.Sequential()` para formar una **neurona log√≠stica completa**.\n",
        "\n",
        "2. **Preparamos los datos (X, Y)**  \n",
        "   - Representamos la tabla OR:\n",
        "\n",
        "      | \\(x_1\\) | \\(x_2\\) | \\(y\\) |\n",
        "      |:---:|:---:|:---:|\n",
        "      | 0 | 0 | 0 |\n",
        "      | 0 | 1 | 1 |\n",
        "      | 1 | 0 | 1 |\n",
        "      | 1 | 1 | 1 |\n",
        "\n",
        "   - Cada fila es un ejemplo de entrenamiento.  \n",
        "   - Los tensores se declaran con `dtype=torch.float32` para operar correctamente con funciones continuas.\n",
        "\n",
        "3. **Funci√≥n de p√©rdida**\n",
        "   - Se usa **Binary Cross Entropy (BCE)**:\n",
        "     $$\n",
        "     L = -\\frac{1}{N} \\sum_i \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i)\\log(1 - \\hat{y}_i) \\right]\n",
        "     $$\n",
        "   - Minimizar esta p√©rdida equivale a **maximizar la log-verosimilitud** en la regresi√≥n log√≠stica.\n",
        "\n",
        "4. **Optimizador (gradiente descendente)**  \n",
        "   - `torch.optim.SGD` actualiza los par√°metros siguiendo la regla:\n",
        "\n",
        "     $$\n",
        "     \\theta \\;:=\\; \\theta \\;-\\; \\eta \\, \\nabla_\\theta L\n",
        "     $$\n",
        "\n",
        "     donde:  \n",
        "     - $\\eta$ ‚Üí *learning rate* (tasa de aprendizaje)  \n",
        "     - $\\nabla_\\theta L$ ‚Üí gradiente de la p√©rdida respecto a los par√°metros del modelo  \n",
        "\n",
        "     üí° En cada paso, el optimizador **ajusta los pesos** en la direcci√≥n que **reduce la p√©rdida**, aplicando el concepto te√≥rico del **m√©todo del gradiente** que vimos en clase.\n",
        "\n",
        "5. **Entrenamiento (ciclo de aprendizaje)**  \n",
        "   En cada iteraci√≥n del entrenamiento, el proceso sigue estos pasos:\n",
        "\n",
        "   1. Calculamos las predicciones: $ \\hat{y} = \\sigma(WX + b) $\n",
        "   2. Calculamos la funci√≥n de p√©rdida: $L(\\hat{y}, y)$\n",
        "   3. Vaciamos gradientes previos con `zero_grad()`  \n",
        "   4. Calculamos los gradientes mediante `backward()`  \n",
        "   5. Actualizamos los par√°metros con `step()`  \n",
        "\n",
        "   üß† Este ciclo es la **implementaci√≥n pr√°ctica del gradiente descendente**:  \n",
        "   el modelo ajusta sus pesos poco a poco hasta minimizar la p√©rdida y aprender la relaci√≥n correcta entre las entradas y las salidas.\n",
        "\n",
        "\n",
        "6. **Resultados**\n",
        "   - Despu√©s de ~10,000 iteraciones, la p√©rdida se aproxima a cero.\n",
        "   - El modelo predice:\n",
        "\n",
        "      | $(x_1)$ | $(x_2)$ | $(y_{real})$ | $(\\hat{y}_{pred})$ esperado |\n",
        "      |:---:|:---:|:---:|:---:|\n",
        "      | 0 | 0 | 0 | ‚âà 0.0 |\n",
        "      | 0 | 1 | 1 | ‚âà 0.99 |\n",
        "      | 1 | 0 | 1 | ‚âà 0.99 |\n",
        "      | 1 | 1 | 1 | ‚âà 1.0 |\n",
        "\n",
        "   - Los pesos $( w_1, w_2 )$ son positivos y el sesgo $( b )$ negativo,  \n",
        "     lo que genera una frontera de decisi√≥n que activa la salida (‚âà1) cuando al menos una entrada es 1.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† ¬øC√≥mo lo hace PyTorch?\n",
        "- **Autograd:** PyTorch crea un *grafo computacional* que rastrea todas las operaciones sobre tensores con `requires_grad=True`.  \n",
        "  Al llamar `loss.backward()`, el sistema calcula autom√°ticamente los gradientes de cada par√°metro.\n",
        "  \n",
        "- **Optimizador:** Usa los gradientes para ajustar \\( w_1, w_2, b \\), reduciendo la p√©rdida en cada paso.  \n",
        "  Este proceso repite el concepto te√≥rico del **gradiente descendente** que viste en clase.\n",
        "\n",
        "---\n",
        "\n",
        "## üìà Resultados esperados\n",
        "- **Predicciones**: el modelo aprende perfectamente la l√≥gica OR.  \n",
        "- **P√©rdida**: desciende hasta valores muy peque√±os (~0.001).  \n",
        "- **Pesos aprendidos**:\n",
        "  $$\n",
        "  w_1, w_2 > 0, \\quad b < 0\n",
        "  $$\n",
        "  indicando que basta una entrada activa (1) para obtener una probabilidad alta.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usando dispositivo: cpu\n",
            "Iteraci√≥n 0 - P√©rdida: 0.7678\n",
            "Iteraci√≥n 1000 - P√©rdida: 0.0898\n",
            "Iteraci√≥n 2000 - P√©rdida: 0.0471\n",
            "Iteraci√≥n 3000 - P√©rdida: 0.0315\n",
            "Iteraci√≥n 4000 - P√©rdida: 0.0236\n",
            "Iteraci√≥n 5000 - P√©rdida: 0.0188\n",
            "Iteraci√≥n 6000 - P√©rdida: 0.0156\n",
            "Iteraci√≥n 7000 - P√©rdida: 0.0134\n",
            "Iteraci√≥n 8000 - P√©rdida: 0.0117\n",
            "Iteraci√≥n 9000 - P√©rdida: 0.0104\n",
            "\n",
            "üîπ Predicciones despu√©s de entrenar:\n",
            "tensor([[0.0205],\n",
            "        [0.9918],\n",
            "        [0.9918],\n",
            "        [1.0000]], grad_fn=<SigmoidBackward0>)\n",
            "\n",
            "üîπ Par√°metros aprendidos:\n",
            "0.weight: tensor([[8.6612, 8.6605]])\n",
            "0.bias: tensor([-3.8653])\n"
          ]
        }
      ],
      "source": [
        "# ====================================================\n",
        "# üß© Neurona Log√≠stica para la funci√≥n OR (PyTorch - CPU)\n",
        "# ====================================================\n",
        "# Objetivo:\n",
        "# Entrenar una neurona log√≠stica (regresi√≥n log√≠stica binaria) para que aprenda\n",
        "# la tabla l√≥gica OR usando PyTorch, sin necesidad de GPU.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Elegir el dispositivo de c√≥mputo\n",
        "# ----------------------------------------------------\n",
        "# torch.device() permite definir si el modelo y los tensores se ejecutan en:\n",
        "#  - \"cuda\" ‚Üí GPU (si est√° disponible)\n",
        "#  - \"cpu\"  ‚Üí procesador normal\n",
        "# torch.cuda.is_available() devuelve True si hay GPU compatible con CUDA.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 1Ô∏è‚É£ Definici√≥n de la capa lineal (modelo lineal)\n",
        "# ----------------------------------------------------\n",
        "# nn.Linear(in_features, out_features)\n",
        "# Crea una transformaci√≥n lineal:  y = X¬∑W^T + b\n",
        "#  - in_features = n√∫mero de variables de entrada (x1, x2 ‚Üí 2)\n",
        "#  - out_features = n√∫mero de salidas (una sola neurona ‚Üí 1)\n",
        "# Internamente guarda dos par√°metros entrenables:\n",
        "#  - weight ‚Üí matriz de pesos (w1, w2)\n",
        "#  - bias   ‚Üí t√©rmino independiente b\n",
        "capa_lineal = nn.Linear(2, 1)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 2Ô∏è‚É£ Funci√≥n de activaci√≥n (sigmoide)\n",
        "# ----------------------------------------------------\n",
        "# nn.Sigmoid() aplica la funci√≥n log√≠stica œÉ(z) = 1 / (1 + e^(-z))\n",
        "# Convierte el resultado lineal (z) en una probabilidad entre 0 y 1.\n",
        "# No recibe argumentos.\n",
        "activacion = nn.Sigmoid()\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 3Ô∏è‚É£ Construcci√≥n del modelo secuencial\n",
        "# ----------------------------------------------------\n",
        "# nn.Sequential() agrupa varias capas en orden:\n",
        "#   - Primero aplica la capa lineal (z = WX + b)\n",
        "#   - Luego la sigmoide (œÉ(z))\n",
        "# to(device) mueve el modelo al CPU o GPU definido antes.\n",
        "modelo = nn.Sequential(\n",
        "    capa_lineal,\n",
        "    activacion\n",
        ").to(device)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 4Ô∏è‚É£ Datos de entrenamiento (Tabla OR)\n",
        "# ----------------------------------------------------\n",
        "# torch.tensor(data, device, dtype)\n",
        "#   - data: lista o arreglo de valores num√©ricos\n",
        "#   - device: en qu√© dispositivo se guardan (CPU o GPU)\n",
        "#   - dtype: tipo de dato (float32 ‚Üí n√∫mero decimal)\n",
        "#\n",
        "# Entradas X: todas las combinaciones de (x1, x2)\n",
        "# Salidas Y: resultados de x1 OR x2\n",
        "X = torch.tensor([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]\n",
        "], device=device, dtype=torch.float32)\n",
        "\n",
        "Y = torch.tensor([\n",
        "    [0],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1]\n",
        "], device=device, dtype=torch.float32)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 5Ô∏è‚É£ Funci√≥n de p√©rdida y optimizador\n",
        "# ----------------------------------------------------\n",
        "# nn.BCELoss() ‚Üí Binary Cross Entropy Loss\n",
        "#   - Calcula la diferencia entre las predicciones y los valores reales.\n",
        "#   - Mide qu√© tan buena es la predicci√≥n de una probabilidad binaria.\n",
        "#   - F√≥rmula: L = -[y¬∑log(≈∑) + (1 - y)¬∑log(1 - ≈∑)]\n",
        "fn_perdida = nn.BCELoss()\n",
        "\n",
        "# torch.optim.SGD(params, lr)\n",
        "#   - Implementa el m√©todo del Gradiente Descendente Estoc√°stico.\n",
        "#   - params: par√°metros del modelo (pesos y sesgos)\n",
        "#   - lr: tasa de aprendizaje (learning rate)\n",
        "# Actualiza los par√°metros del modelo en cada iteraci√≥n:\n",
        "#   Œ∏ := Œ∏ - Œ∑ * ‚àáŒ∏ L\n",
        "optimizador = torch.optim.SGD(modelo.parameters(), lr=0.1)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 6Ô∏è‚É£ Ciclo de entrenamiento\n",
        "# ----------------------------------------------------\n",
        "# Repite el proceso 10,000 veces para que el modelo aprenda (epochs)\n",
        "for epoch in range(10000):\n",
        "\n",
        "    # --- Forward pass ---\n",
        "    # Calcula las salidas del modelo para las entradas X\n",
        "    # Internamente: ≈∑ = œÉ(WX + b)\n",
        "    y_pred = modelo(X)\n",
        "\n",
        "    # --- Calcular p√©rdida ---\n",
        "    # Compara las predicciones (y_pred) con los valores reales (Y)\n",
        "    perdida = fn_perdida(y_pred, Y)\n",
        "    \n",
        "    # --- Limpiar gradientes ---\n",
        "    # Antes de calcular nuevos gradientes, se limpian los anteriores\n",
        "    optimizador.zero_grad()\n",
        "\n",
        "    # --- Backpropagation ---\n",
        "    # Calcula autom√°ticamente las derivadas parciales (gradientes)\n",
        "    # de la p√©rdida con respecto a cada par√°metro del modelo\n",
        "    perdida.backward()\n",
        "\n",
        "    # --- Actualizaci√≥n de par√°metros ---\n",
        "    # Modifica los pesos (w1, w2) y el sesgo (b)\n",
        "    # seg√∫n la direcci√≥n del gradiente y la tasa de aprendizaje\n",
        "    optimizador.step()\n",
        "    \n",
        "    # --- Monitoreo del entrenamiento ---\n",
        "    # Cada 1000 iteraciones imprime la p√©rdida para ver la convergencia\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Iteraci√≥n {epoch} - P√©rdida: {perdida.item():.4f}\")\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 7Ô∏è‚É£ Predicciones finales\n",
        "# ----------------------------------------------------\n",
        "# Despu√©s del entrenamiento, volvemos a pasar los datos por el modelo\n",
        "# para ver qu√© probabilidades produce.\n",
        "print(\"\\nüîπ Predicciones despu√©s de entrenar:\")\n",
        "print(modelo(X))\n",
        "# Deber√≠an aproximarse a:\n",
        "# [ [0.0], [0.99], [0.99], [1.0] ]\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 8Ô∏è‚É£ Par√°metros finales (pesos y sesgo)\n",
        "# ----------------------------------------------------\n",
        "# modelo.named_parameters() devuelve un iterable con:\n",
        "#  - el nombre del par√°metro (\"weight\" o \"bias\")\n",
        "#  - el valor entrenado (tensor con los pesos aprendidos)\n",
        "print(\"\\nüîπ Par√°metros aprendidos:\")\n",
        "for nombre, param in modelo.named_parameters():\n",
        "    print(f\"{nombre}: {param.data}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Resumen de las funciones usadas\n",
        "\n",
        "- **`torch.device(type)`**  \n",
        "  Define si se usar√° **CPU** o **GPU** para ejecutar el modelo y los tensores.  \n",
        "  Argumento: `\"cpu\"` o `\"cuda\"`.\n",
        "\n",
        "---\n",
        "\n",
        "- **`torch.cuda.is_available()`**  \n",
        "  Devuelve `True` si existe una **GPU compatible con CUDA** disponible en el sistema.  \n",
        "  *(Sin argumentos).*\n",
        "\n",
        "---\n",
        "\n",
        "- **`nn.Linear(in_features, out_features)`**  \n",
        "  Crea una capa lineal que aplica la transformaci√≥n:  \n",
        "  $$\n",
        "  y = W X + b\n",
        "  $$\n",
        "  donde \\( W \\) son los **pesos** y \\( b \\) el **sesgo**.  \n",
        "  Argumentos:  \n",
        "  - `in_features`: n√∫mero de entradas (columnas de \\( X \\)).  \n",
        "  - `out_features`: n√∫mero de salidas (neuronas de salida).\n",
        "\n",
        "---\n",
        "\n",
        "- **`nn.Sigmoid()`**  \n",
        "  Aplica la funci√≥n log√≠stica:  \n",
        "  $$\n",
        "  \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "  $$\n",
        "  Convierte un valor real \\( z \\) en una probabilidad entre 0 y 1.  \n",
        "  *(Sin argumentos).*\n",
        "\n",
        "---\n",
        "\n",
        "- **`nn.Sequential(...)`**  \n",
        "  Combina varias capas en orden, de modo que la salida de una es la entrada de la siguiente.  \n",
        "  Argumento: lista de capas (por ejemplo `[nn.Linear(), nn.Sigmoid()]`).\n",
        "\n",
        "---\n",
        "\n",
        "- **`torch.tensor(data, device, dtype)`**  \n",
        "  Crea un **tensor** (estructura de datos multidimensional).  \n",
        "  Argumentos:  \n",
        "  - `data`: lista o arreglo NumPy.  \n",
        "  - `device`: `\"cpu\"` o `\"cuda\"`.  \n",
        "  - `dtype`: tipo de dato, por ejemplo `torch.float32`.\n",
        "\n",
        "---\n",
        "\n",
        "- **`nn.BCELoss()`**  \n",
        "  Calcula la **p√©rdida de entrop√≠a cruzada binaria**, que mide la diferencia entre las probabilidades predichas y las reales:  \n",
        "  $$\n",
        "  L = -[y \\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})]\n",
        "  $$\n",
        "  *(Sin argumentos).*\n",
        "\n",
        "---\n",
        "\n",
        "- **`torch.optim.SGD(params, lr)`**  \n",
        "  Implementa el **Gradiente Descendente Estoc√°stico (SGD)**, actualizando los par√°metros seg√∫n:  \n",
        "  $$\n",
        "  \\theta := \\theta - \\eta \\nabla_\\theta L\n",
        "  $$\n",
        "  Argumentos:  \n",
        "  - `params`: par√°metros del modelo (pesos y sesgos).  \n",
        "  - `lr`: tasa de aprendizaje (\\( \\eta \\)).\n",
        "\n",
        "---\n",
        "\n",
        "- **`optimizer.zero_grad()`**  \n",
        "  Limpia los **gradientes acumulados** antes de calcular los nuevos.  \n",
        "  *(Sin argumentos).*\n",
        "\n",
        "---\n",
        "\n",
        "- **`loss.backward()`**  \n",
        "  Calcula los **gradientes** de la p√©rdida con respecto a cada par√°metro del modelo usando *backpropagation*.  \n",
        "  *(Sin argumentos).*\n",
        "\n",
        "---\n",
        "\n",
        "- **`optimizer.step()`**  \n",
        "  Actualiza los par√°metros (\\( W \\), \\( b \\)) usando los gradientes calculados.  \n",
        "  *(Sin argumentos).*\n",
        "\n",
        "---\n",
        "\n",
        "- **`model(X)`**  \n",
        "  Aplica el **modelo completo** (capa lineal + sigmoide) a las entradas \\( X \\) y devuelve las predicciones \\( \\hat{y} \\).  \n",
        "  Argumento: `X` ‚Äî tensor con los datos de entrada.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
