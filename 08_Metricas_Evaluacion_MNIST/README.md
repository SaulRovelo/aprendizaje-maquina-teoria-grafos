# üìä M√©tricas de Evaluaci√≥n del Modelo (MNIST 0 y 1)

Este notebook ampl√≠a el proyecto anterior de la **red neuronal para reconocer los d√≠gitos 0 y 1** del conjunto MNIST, incorporando ahora un paso esencial del flujo de aprendizaje autom√°tico: **la evaluaci√≥n del modelo mediante m√©tricas**.

---

## üß† ¬øQu√© son las m√©tricas de evaluaci√≥n?

Las **m√©tricas de evaluaci√≥n** son indicadores que permiten medir el **desempe√±o real de un modelo entrenado**.  
No basta con entrenar y obtener una p√©rdida baja: tambi√©n es necesario **verificar qu√© tan bien el modelo predice en los datos de prueba** y c√≥mo se comporta en cada clase.

En esta pr√°ctica se utilizan tres m√©tricas fundamentales:

### üîπ Accuracy (Precisi√≥n global)
Indica el **porcentaje total de aciertos** del modelo.  
Se calcula comparando todas las predicciones con las etiquetas reales:

$$[
Accuracy = \frac{\text{Predicciones correctas}}{\text{Total de muestras}} \times 100
]
$$

Es una medida general, pero no distingue entre clases desbalanceadas.

---

### üîπ Reporte de Clasificaci√≥n
Generado con `classification_report()` de *scikit-learn*, muestra tres m√©tricas por clase:

- **Precision:** porcentaje de aciertos sobre todas las predicciones positivas.  
- **Recall:** porcentaje de verdaderos positivos detectados correctamente.  
- **F1-score:** promedio arm√≥nico entre precisi√≥n y recall, equilibra ambos valores.

Esta herramienta permite identificar **si el modelo favorece una clase sobre otra**.

---

### üîπ Matriz de Confusi√≥n
Generada con `confusion_matrix()`, muestra los **aciertos y errores por clase** en forma de tabla:

|               | Predicci√≥n 0 | Predicci√≥n 1 |
|---------------|--------------|--------------|
| **Real 0**    | Verdaderos negativos | Falsos positivos |
| **Real 1**    | Falsos negativos | Verdaderos positivos |

Se visualiza con un *heatmap* mediante **Seaborn**, facilitando la interpretaci√≥n de los resultados.

---

## üß© Conclusi√≥n

Las m√©tricas de evaluaci√≥n son esenciales para **interpretar el rendimiento real del modelo**, detectar sesgos y mejorar futuras versiones.  
Gracias a estas herramientas, podemos ir m√°s all√° del simple ‚Äúacierta o falla‚Äù y entender **c√≥mo y por qu√© el modelo toma sus decisiones**.

---
